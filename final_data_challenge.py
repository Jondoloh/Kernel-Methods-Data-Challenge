# -*- coding: utf-8 -*-
"""Final Data Challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhZQvcNr84xUWaAtJn40vyuJbbdzP6fa

# ----------------- Data challenge for the AMMI 2022 course "Kernel Methods" ------------------


                                                                       by:


                                                            John Ndolo & Yvan Pimi

#### Introduction :
###### The goal of the data challenge is to learn how to implement machine learning algorithms, gain understanding about them and adapt them to structural data.For this reason, we have chosen a sequence classification task: predicting whether a DNA sequence (or read) belongs to the SARS-CoV-2 (Covid-19).

#### Data description :
###### Both the training and evaluation data are sets of DNA sequencing reads: short DNA fragments (~100 to 300bp long), that come from sequencing experiments, or that were simulated from full genomes. Some of these fragments come from Covid-19 genomes, others from human or random bacteria. The goal is to discriminate the Covid-19 fragments, hence the task is a binary classification task: the labels are either 1 if the fragment is identified as Covid-19, and 0 otherwise.

## 1.Importing dependencies
"""

#Hide warnings
import warnings
warnings.simplefilter('ignore')

#Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## 2. Loading Data"""

#dataset
X_train =pd.read_csv('data/Xtr_vectors.csv',index_col=0)
#X_train =pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kernel_Data_Challenge/Xtr.csv',index_col=0)
Y_train = pd.read_csv('data/Ytr.csv',index_col=0)
#X_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Kernel_Data_Challenge/Xte.csv',index_col=0)
X_test = pd.read_csv('data/Xte_vectors.csv',index_col=0)

X_train.shape,X_test.shape,Y_train.shape

Y_train=(2*Y_train)-1
#Y_train

"""## 3.Model Selection

*< A guide trial using Scikit learn and other defined libraries >*
"""

X = X_train.values.astype('float')
Y = Y_train.values.flatten().astype('float')
x_test = X_test.values.astype('float')

X.shape,Y.shape,x_test.shape

from sklearn import svm
from sklearn.metrics import classification_report, accuracy_score

# Define scoring method
scoring = 'accuracy'
# Model building to train
names = ['SVM Linear','SVM RBF','SVM Sigmoid']

Classifiers = [
               svm.SVC(kernel = 'linear'),
               svm.SVC(kernel = 'rbf'),
               svm.SVC(kernel = 'sigmoid')
               ]
models = zip(names, Classifiers)
# import KFold
from sklearn.model_selection import KFold, cross_val_score

names = []
result = []
for name, model in models:
    kfold = KFold(n_splits = 10)
    cv_results = cross_val_score(model, X, Y, cv = kfold, scoring = 'accuracy')
    result.append(cv_results)
    names.append(name)
    msg = "{0}: {1} ({2})".format(name, cv_results.mean(), cv_results.std())
    print(msg)

#Test the algorithm on the test data set
models = zip(names, Classifiers)
for name, model in models:
    model.fit(X, Y)
    y_pred = model.predict(x_test)
    print(name)

"""## Settling on Kernel SVM (Linear and rbf kernels)

-Linear SVM works better in this case-the final model adpated

-Building our Model (Codes -adapted from Lab 6)
"""

import cvxopt

def cvxopt_qp(P, q, G, h, A, b):
    P = .5 * (P + P.T)
    cvx_matrices = [
        cvxopt.matrix(M) if M is not None else None for M in [P, q, G, h, A, b] 
    ]
    #cvxopt.solvers.options['show_progress'] = False
    solution = cvxopt.solvers.qp(*cvx_matrices, options={'show_progress': False})
    return np.array(solution['x']).flatten()

solve_qp = cvxopt_qp

def add_column_ones(X):
    n = X.shape[0]
    return np.hstack([X, np.ones((n, 1))])

def rbf_kernel(X1, X2, sigma=10):
    '''
    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)
    where K is the RBF kernel with parameter sigma
    
    Input:
    ------
    X1: an (n1, p) matrix
    X2: an (n2, p) matrix
    sigma: float
    '''
    # For loop with rbf_kernel_element works but is slow in python
    # Use matrix operations!
    X2_norm = np.sum(X2 ** 2, axis = -1)
    X1_norm = np.sum(X1 ** 2, axis = -1)
    gamma = 1 / (2 * sigma ** 2)
    K = np.exp(- gamma * (X1_norm[:, None] + X2_norm[None, :] - 2 * np.dot(X1, X2.T)))
    return K

def sigma_from_median(X):
    '''
    Returns the median of ||Xi-Xj||
    
    Input
    -----
    X: (n, p) matrix
    '''
    pairwise_diff = X[:, :, None] - X[:, :, None].T
    pairwise_diff *= pairwise_diff
    euclidean_dist = np.sqrt(pairwise_diff.sum(axis=1))
    return np.median(euclidean_dist)

"""#### Examples of kernels"""

def linear_kernel(X1, X2):
    '''
    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)
    where K is the linear kernel
    
    Input:
    ------
    X1: an (n1, p) matrix
    X2: an (n2, p) matrix
    '''
    return X1@X2.T


def polynomial_kernel(X1, X2, degree=2):
    '''
    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)
    where K is the polynomial kernel of degree `degree`
    
    Input:
    ------
    X1: an (n1, p) matrix
    X2: an (n2, p) matrix
    '''
    return (1+linear_kernel(X1,X2))**2

class KernelMethodBase(object):
    '''
    Base class for kernel methods models
    
    Methods
    ----
    fit
    predict
    fit_K
    predict_K
    '''
    kernels_ = {
        'linear': linear_kernel,
        'polynomial': polynomial_kernel,
        'rbf': rbf_kernel,
        # 'mismatch': mismatch_kernel,
    }
    def __init__(self, kernel='linear', **kwargs):
        self.kernel_name = kernel
        self.kernel_function_ = self.kernels_[kernel]
        self.kernel_parameters = self.get_kernel_parameters(**kwargs)
        self.fit_intercept_ = False
        
    def get_kernel_parameters(self, **kwargs):
        params = {}
        if self.kernel_name == 'rbf':
            params['sigma'] = kwargs.get('sigma', 1.)
        if self.kernel_name == 'polynomial':
            params['degree'] = kwargs.get('degree', 2)
        return params

    def fit_K(self, K, y, **kwargs):
        pass
        
    def decision_function_K(self, K):
        pass
    
    def fit(self, X, y, fit_intercept=False, **kwargs):

        if fit_intercept:
            X = add_column_ones(X)
            self.fit_intercept_ = True
        self.X_train = X
        self.y_train = y

        K = self.kernel_function_(self.X_train, self.X_train, **self.kernel_parameters)

        return self.fit_K(K, y, **kwargs)
    
    def decision_function(self, X):

        if self.fit_intercept_:
            X = add_column_ones(X)

        K_x = self.kernel_function_(X, self.X_train, **self.kernel_parameters)

        return self.decision_function_K(K_x)

    def predict(self, X):
        pass
    
    def predict_K(self, K):
        pass

def svm_dual_soft_to_qp_kernel(K, y, C=1):
    n = K.shape[0]
    assert (len(y) == n)
        
    # Dual formulation, soft margin
    P = P = np.diag(y)@K@np.diag(y)
    # As a regularization, we add epsilon * identity to P
    eps = 1e-12
    P += eps * np.eye(n)
    q = - np.ones(n)
    G = np.vstack([-np.eye(n), np.eye(n)])
    h = np.hstack([np.zeros(n), C * np.ones(n)])
    A = y[np.newaxis, :]
    b = np.array([0.])
    return P, q, G, h, A, b

K = linear_kernel(X, X)
alphas = solve_qp(*svm_dual_soft_to_qp_kernel(K, Y, C=1.))

class KernelSVM(KernelMethodBase):
    '''
    Kernel SVM Classification
    
    Methods
    ----
    fit
    predict
    '''
    def __init__(self, C=0.1, **kwargs):
        self.C = C
        super().__init__(**kwargs)
    
    def fit_K(self, K, y, tol=1e-3):
        # Solve dual problem
        self.alpha = solve_qp(*svm_dual_soft_to_qp_kernel(K, y, C=self.C))
        
        # Compute support vectors and bias b
        sv = np.logical_and((self.alpha > tol), (self.C - self.alpha > tol))
        self.bias = y[sv] - K[sv].dot(self.alpha * y)
        self.bias = self.bias.mean()

        self.support_vector_indices = np.nonzero(sv)[0]
        self.beta = self.alpha * y

        return self
        
    def decision_function_K(self, K_x):
        # print('K', K_x.shape, 'alpha', self.alpha.shape, 'bias', self.bias)
        return K_x@(self.alpha * self.beta) +self.bias
        # return K_x.dot(self.alpha * self.y_train) + self.bias

    def predict(self, X):
         #return np.sign(self.decision_function(X))
        return np.where(self.decision_function(X)>0.5,1,-1)

"""#### Parameters tuning"""

kernel = 'linear'
sigma = 1.
degree = 2
C = 1.5
tol = 1e-3
model = KernelSVM(C=C, kernel=kernel, sigma=sigma, degree=degree)
y_pred = model.fit(X, Y, tol=tol).predict(x_test)

y_pred

#converting to desired format in our submission file
y_pred_0 = np.where(y_pred==1,1,0)
y_pred_0

"""### Saving submission file"""

y_save = np.vstack([1 + np.arange(len(y_pred_0)), y_pred_0]).T
y_save[:10]

# Save as a csv file
np.savetxt('Yte.csv', y_save,
           delimiter=',', header='Id,Covid', fmt='%i', comments='')